{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook prepares the data for LoRA vs full finetuning analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import textwrap\n",
    "import random \n",
    "import ast\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora_path = \"../results/pythia-1.4b/lora/r_16/lr_2e-4/early_stopping/num_train_4096/bsize_128/tkn_freq_probs_final.csv\"\n",
    "lora_path = \"../results/pythia-1.4b/lora/r_16/lr_2e-4/early_stopping/num_train_4096/bsize_128/tkn_freq_probs_best.csv\"\n",
    "lora_df = pd.read_csv(lora_path)\n",
    "\n",
    "# full_path = \"../results/pythia-1.4b/full-ft/lr_2e-6/early_stopping/num_train_4096/bsize_128/tkn_freq_probs_final.csv\"\n",
    "full_path = \"../results/pythia-1.4b/full-ft/lr_2e-6/early_stopping/num_train_4096/bsize_128/tkn_freq_probs_best.csv\"\n",
    "full_df = pd.read_csv(full_path)\n",
    "\n",
    "base_path = \"../results/pythia-1.4b/base_model/num_train_4096/tkn_freq_probs_base.csv\"\n",
    "base_df = pd.read_csv(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                            tensor([4673])\n",
       "1                                      tensor([4673,   75])\n",
       "2                             tensor([ 4673,    75, 13293])\n",
       "3                      tensor([ 4673,    75, 13293,   642])\n",
       "4               tensor([ 4673,    75, 13293,   642,   657])\n",
       "                                ...                        \n",
       "520187    tensor([  775,  1952,   652,  6960,   253,   4...\n",
       "520188    tensor([  775,  1952,   652,  6960,   253,   4...\n",
       "520189    tensor([  775,  1952,   652,  6960,   253,   4...\n",
       "520190    tensor([  775,  1952,   652,  6960,   253,   4...\n",
       "520191    tensor([  775,  1952,   652,  6960,   253,   4...\n",
       "Name: in_token_ids, Length: 520192, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_df[\"in_token_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract context length from df \n",
    "\n",
    "def context_processing(x):\n",
    "    context_ids, token = x[\"in_token_ids\"], x[\"curr_token_id\"]\n",
    "    context = ast.literal_eval(re.split(r'tensor\\(|\\].*', context_ids)[1] + ']')\n",
    "    x[\"context_len\"] = len(context)\n",
    "    x[\"token_in_context\"] = int(token in context)\n",
    "    x[\"uniq_ctxt_tkns_count\"] = len(set(context))\n",
    "    return x\n",
    "    \n",
    "def get_context(x):\n",
    "    return ast.literal_eval(re.split(r'tensor\\(|\\].*', x)[1] + ']')\n",
    "\n",
    "class ContextProcessor():\n",
    "    def __init__(self, df):\n",
    "        token = df[\"curr_token_id\"]\n",
    "        context = df[\"in_token_ids\"].progress_apply(lambda x: get_context(x))\n",
    "        self.df = pd.DataFrame({\"context\": context, \"token\": token})\n",
    "\n",
    "    def get_context_len(self):\n",
    "        return self.df.context.apply(len)\n",
    "    \n",
    "    def is_token_in_context(self):\n",
    "        token_in_context = self.df.apply(lambda x: int(x[\"token\"] in x[\"context\"]), axis=1)\n",
    "        return token_in_context\n",
    "    \n",
    "    def uniq_ctxt_tkns_count(self): \n",
    "        return self.df.context.apply(lambda x: len(set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cp = ContextProcessor(lora_df[:100])\n",
    "\n",
    "# lora_df[\"context_len\"] = cp.get_context_len()\n",
    "# lora_df[\"token_in_context\"] = cp.is_token_in_context()\n",
    "# lora_df[\"uniq_ctxt_tkns_count\"] = cp.uniq_ctxt_tkns_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_df = lora_df.apply(context_processing, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df.apply(context_processing, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prev_token</th>\n",
       "      <th>curr_token</th>\n",
       "      <th>prev_token_id</th>\n",
       "      <th>curr_token_id</th>\n",
       "      <th>in_tokens</th>\n",
       "      <th>in_token_ids</th>\n",
       "      <th>prev_token_freq</th>\n",
       "      <th>curr_token_freq</th>\n",
       "      <th>pair_token_freq</th>\n",
       "      <th>curr_token_prob</th>\n",
       "      <th>pmi</th>\n",
       "      <th>context_len</th>\n",
       "      <th>token_in_context</th>\n",
       "      <th>uniq_ctxt_tkns_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ĠSen</td>\n",
       "      <td>j</td>\n",
       "      <td>4673</td>\n",
       "      <td>75</td>\n",
       "      <td>Sen</td>\n",
       "      <td>tensor([4673])</td>\n",
       "      <td>8</td>\n",
       "      <td>119</td>\n",
       "      <td>6</td>\n",
       "      <td>0.036710</td>\n",
       "      <td>-0.850398</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>j</td>\n",
       "      <td>Åį</td>\n",
       "      <td>75</td>\n",
       "      <td>13293</td>\n",
       "      <td>Senj</td>\n",
       "      <td>tensor([4673,   75])</td>\n",
       "      <td>119</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>0.192568</td>\n",
       "      <td>-2.236693</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Åį</td>\n",
       "      <td>Ġno</td>\n",
       "      <td>13293</td>\n",
       "      <td>642</td>\n",
       "      <td>Senjō</td>\n",
       "      <td>tensor([ 4673,    75, 13293])</td>\n",
       "      <td>32</td>\n",
       "      <td>267</td>\n",
       "      <td>6</td>\n",
       "      <td>0.021627</td>\n",
       "      <td>-3.044818</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ġno</td>\n",
       "      <td>ĠV</td>\n",
       "      <td>642</td>\n",
       "      <td>657</td>\n",
       "      <td>Senjō no</td>\n",
       "      <td>tensor([ 4673,    75, 13293,   642])</td>\n",
       "      <td>267</td>\n",
       "      <td>247</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>-4.241172</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ĠV</td>\n",
       "      <td>alk</td>\n",
       "      <td>657</td>\n",
       "      <td>1278</td>\n",
       "      <td>Senjō no V</td>\n",
       "      <td>tensor([ 4673,    75, 13293,   642,   657])</td>\n",
       "      <td>247</td>\n",
       "      <td>29</td>\n",
       "      <td>55</td>\n",
       "      <td>0.040921</td>\n",
       "      <td>-0.652944</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  prev_token curr_token  prev_token_id  curr_token_id    in_tokens  \\\n",
       "0       ĠSen          j           4673             75          Sen   \n",
       "1          j         Åį             75          13293         Senj   \n",
       "2         Åį        Ġno          13293            642        Senjō   \n",
       "3        Ġno         ĠV            642            657     Senjō no   \n",
       "4         ĠV        alk            657           1278   Senjō no V   \n",
       "\n",
       "                                  in_token_ids  prev_token_freq  \\\n",
       "0                               tensor([4673])                8   \n",
       "1                         tensor([4673,   75])              119   \n",
       "2                tensor([ 4673,    75, 13293])               32   \n",
       "3         tensor([ 4673,    75, 13293,   642])              267   \n",
       "4  tensor([ 4673,    75, 13293,   642,   657])              247   \n",
       "\n",
       "   curr_token_freq  pair_token_freq  curr_token_prob       pmi  context_len  \\\n",
       "0              119                6         0.036710 -0.850398            1   \n",
       "1               32                6         0.192568 -2.236693            2   \n",
       "2              267                6         0.021627 -3.044818            3   \n",
       "3              247               14         0.000253 -4.241172            4   \n",
       "4               29               55         0.040921 -0.652944            5   \n",
       "\n",
       "   token_in_context  uniq_ctxt_tkns_count  \n",
       "0                 0                     1  \n",
       "1                 0                     2  \n",
       "2                 0                     3  \n",
       "3                 0                     4  \n",
       "4                 0                     5  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_df = lora_df.rename(columns={\"curr_token_prob\": \"lora_prob\"})\n",
    "full_df = full_df.rename(columns={\"curr_token_prob\": \"full_prob\"})\n",
    "base_df = base_df.rename(columns={\"curr_token_prob\": \"base_prob\"})\n",
    "common_cols = set(lora_df.columns).intersection(set(full_df.columns))\n",
    "full_df.drop(common_cols, axis=1, inplace=True)\n",
    "common_cols = common_cols.intersection(set(base_df.columns))\n",
    "base_df.drop(common_cols, axis=1, inplace=True)\n",
    "df_combined = pd.concat([lora_df, full_df, base_df], axis=1)\n",
    "df_combined[\"full_lora_diff\"] = df_combined.full_prob - df_combined.lora_prob\n",
    "df_combined[\"lora_base_diff\"] = df_combined.lora_prob - df_combined.base_prob\n",
    "df_combined[\"full_base_diff\"] = df_combined.full_prob - df_combined.base_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>curr_token</th>\n",
       "      <th>in_tokens</th>\n",
       "      <th>context_len</th>\n",
       "      <th>token_in_context</th>\n",
       "      <th>uniq_ctxt_tkns_count</th>\n",
       "      <th>prev_token</th>\n",
       "      <th>pmi</th>\n",
       "      <th>curr_token_freq</th>\n",
       "      <th>prev_token_freq</th>\n",
       "      <th>pair_token_freq</th>\n",
       "      <th>lora_prob</th>\n",
       "      <th>full_prob</th>\n",
       "      <th>base_prob</th>\n",
       "      <th>full_lora_diff</th>\n",
       "      <th>lora_base_diff</th>\n",
       "      <th>full_base_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>Ġ@</td>\n",
       "      <td>Mogadishu University ( MU ) is a non</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>Ġnon</td>\n",
       "      <td>-4.023</td>\n",
       "      <td>5438</td>\n",
       "      <td>55</td>\n",
       "      <td>79</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.918</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.</td>\n",
       "      <td>Del Toso was a 4 point wheelchair basketball ...</td>\n",
       "      <td>113</td>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>Ġ@</td>\n",
       "      <td>-3.831</td>\n",
       "      <td>1822</td>\n",
       "      <td>5438</td>\n",
       "      <td>3170</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.865</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9079</th>\n",
       "      <td>Ġold</td>\n",
       "      <td>18 @-@ year @-@</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>@</td>\n",
       "      <td>-4.215</td>\n",
       "      <td>101</td>\n",
       "      <td>5363</td>\n",
       "      <td>118</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.858</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Hg</td>\n",
       "      <td>A disturbance in the ITCZ developed into a tr...</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>Ġin</td>\n",
       "      <td>-4.198</td>\n",
       "      <td>12</td>\n",
       "      <td>9025</td>\n",
       "      <td>24</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.773</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11609</th>\n",
       "      <td>Ġtrack</td>\n",
       "      <td>Lost Horizons is the second studio album from...</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>Ġout</td>\n",
       "      <td>-3.966</td>\n",
       "      <td>74</td>\n",
       "      <td>435</td>\n",
       "      <td>9</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.767</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      curr_token                                          in_tokens  \\\n",
       "1181          Ġ@               Mogadishu University ( MU ) is a non   \n",
       "2              .   Del Toso was a 4 point wheelchair basketball ...   \n",
       "9079        Ġold                                    18 @-@ year @-@   \n",
       "64            Hg   A disturbance in the ITCZ developed into a tr...   \n",
       "11609     Ġtrack   Lost Horizons is the second studio album from...   \n",
       "\n",
       "       context_len  token_in_context  uniq_ctxt_tkns_count prev_token    pmi  \\\n",
       "1181            12                 0                    12       Ġnon -4.023   \n",
       "2              113                 1                    72         Ġ@ -3.831   \n",
       "9079             8                 0                     5          @ -4.215   \n",
       "64              99                 0                    76        Ġin -4.198   \n",
       "11609           77                 0                    63       Ġout -3.966   \n",
       "\n",
       "       curr_token_freq  prev_token_freq  pair_token_freq  lora_prob  \\\n",
       "1181              5438               55               79      0.940   \n",
       "2                 1822             5438             3170      0.929   \n",
       "9079               101             5363              118      0.909   \n",
       "64                  12             9025               24      0.956   \n",
       "11609               74              435                9      0.820   \n",
       "\n",
       "       full_prob  base_prob  full_lora_diff  lora_base_diff  full_base_diff  \n",
       "1181       0.022      0.000          -0.918           0.940           0.022  \n",
       "2          0.064      0.014          -0.865           0.915           0.050  \n",
       "9079       0.051      0.001          -0.858           0.909           0.050  \n",
       "64         0.183      0.018          -0.773           0.938           0.165  \n",
       "11609      0.053      0.016          -0.767           0.804           0.037  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_diffs = df_combined.sort_values(\"full_lora_diff\")\n",
    "df_selected = sorted_diffs[[\"in_tokens\", \"context_len\", \"token_in_context\", \"uniq_ctxt_tkns_count\", \"prev_token\", \"curr_token\", \"pmi\", \"curr_token_freq\", \"prev_token_freq\", \"pair_token_freq\", \"lora_prob\", \"full_prob\", \"base_prob\", \"full_lora_diff\", \"lora_base_diff\", \"full_base_diff\"]]\n",
    "finetune_df = df_selected\n",
    "finetune_df = finetune_df.dropna().reset_index(drop=True)\n",
    "df_selected = df_selected[(df_selected[\"pmi\"] > -4.75) & (df_selected[\"pmi\"] < -3.75)]\n",
    "# find unique pairs of w_{i-1}, w_i\n",
    "agg_fns = {c: 'first' for c in df_selected.columns if c not in [\"curr_token\"]}\n",
    "agg_fns[\"full_lora_diff\"] = \"min\"\n",
    "df_uniq = df_selected.groupby([\"curr_token\"]).agg(agg_fns).reset_index().sort_values(\"full_lora_diff\")\n",
    "\n",
    "top_100 = df_uniq[:100]\n",
    "top_100.loc[:, [\"pmi\", \"curr_token_freq\", \"lora_prob\", \"full_prob\", \"base_prob\", \"full_lora_diff\", \"lora_base_diff\", \"full_base_diff\"]] = top_100[[\"pmi\", \"curr_token_freq\", \"lora_prob\", \"full_prob\", \"base_prob\", \"full_lora_diff\", \"lora_base_diff\", \"full_base_diff\"]].round(3)\n",
    "top_100.to_csv(\"results/examples_ft_pt/finetune_data_100.csv\", index=False)\n",
    "top_100.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_df.to_csv(\"results/data/finetune_data_probs.csv\", index=False)\n",
    "# finetune_df = pd.read_csv(\"results/data/finetune_data_probs.csv\")\n",
    "# finetune_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_path = \"../results/pythia-1.4b/lora/r_16/lr_2e-4/early_stopping/pretraining/tkn_freq_probs_best.csv\"\n",
    "lora_df = pd.read_csv(lora_path)\n",
    "full_path = \"../results/pythia-1.4b/full-ft/lr_2e-6/early_stopping/pretraining/tkn_freq_probs_best.csv\"\n",
    "full_df = pd.read_csv(full_path)\n",
    "base_path = \"../results/pythia-1.4b/base_model/pretraining/tkn_freq_probs_base.csv\"\n",
    "base_df = pd.read_csv(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          tensor([1413], dtype=torch.int32)\n",
       "1                    tensor([1413,   27], dtype=torch.int32)\n",
       "2              tensor([1413,   27,   49], dtype=torch.int32)\n",
       "3          tensor([1413,   27,   49,  363], dtype=torch.i...\n",
       "4          tensor([1413,   27,   49,  363, 2721], dtype=t...\n",
       "                                 ...                        \n",
       "2097148    tensor([  273, 32212,   267,  6943, 18334,  11...\n",
       "2097149    tensor([32212,   267,  6943, 18334,  1119, 313...\n",
       "2097150    tensor([  267,  6943, 18334,  1119, 31388,    ...\n",
       "2097151    tensor([ 6943, 18334,  1119, 31388,    71,   5...\n",
       "2097152    tensor([18334,  1119, 31388,    71,   579,   9...\n",
       "Name: in_token_ids, Length: 2097153, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_df[\"in_token_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_df = lora_df.dropna().reset_index(drop=True)\n",
    "cp = ContextProcessor(lora_df)\n",
    "lora_df[\"context_len\"]= cp.get_context_len()\n",
    "lora_df[\"token_in_context\"]= cp.is_token_in_context()\n",
    "lora_df[\"uniq_ctxt_tkns_count\"]= cp.uniq_ctxt_tkns_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 1111028/2096781 [03:07<02:40, 6136.78it/s]IOStream.flush timed out\n",
      "100%|██████████| 2096781/2096781 [06:02<00:00, 5792.12it/s] \n"
     ]
    }
   ],
   "source": [
    "full_df = full_df.dropna().reset_index(drop=True)\n",
    "cp = ContextProcessor(full_df)\n",
    "full_df[\"context_len\"]= cp.get_context_len()\n",
    "full_df[\"token_in_context\"]= cp.is_token_in_context()\n",
    "full_df[\"uniq_ctxt_tkns_count\"]= cp.uniq_ctxt_tkns_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_df = lora_df.rename(columns={\"curr_token_prob\": \"lora_prob\"})\n",
    "full_df = full_df.rename(columns={\"curr_token_prob\": \"full_prob\"})\n",
    "base_df = base_df.rename(columns={\"curr_token_prob\": \"base_prob\"})\n",
    "common_cols = set(lora_df.columns).intersection(set(full_df.columns))\n",
    "full_df.drop(common_cols, axis=1, inplace=True)\n",
    "common_cols = common_cols.intersection(set(base_df.columns))\n",
    "base_df.drop(common_cols, axis=1, inplace=True)\n",
    "df_combined = pd.concat([lora_df, full_df, base_df], axis=1)\n",
    "df_combined[\"full_lora_diff\"] = df_combined.full_prob - df_combined.lora_prob\n",
    "df_combined[\"lora_base_diff\"] = df_combined.lora_prob - df_combined.base_prob\n",
    "df_combined[\"full_base_diff\"] = df_combined.full_prob - df_combined.base_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_diffs = df_combined.sort_values(\"full_lora_diff\", ascending=False)\n",
    "df_selected = sorted_diffs[[\"in_tokens\", \"context_len\", \"token_in_context\", \"uniq_ctxt_tkns_count\", \"prev_token\", \"curr_token\", \"pmi\", \"curr_token_freq\", \"prev_token_freq\", \"pair_token_freq\", \"lora_prob\", \"full_prob\", \"base_prob\", \"full_lora_diff\", \"lora_base_diff\", \"full_base_diff\"]]\n",
    "pretrain_df = df_selected\n",
    "pretrain_df = pretrain_df.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>curr_token</th>\n",
       "      <th>in_tokens</th>\n",
       "      <th>context_len</th>\n",
       "      <th>token_in_context</th>\n",
       "      <th>uniq_ctxt_tkns_count</th>\n",
       "      <th>prev_token</th>\n",
       "      <th>pmi</th>\n",
       "      <th>curr_token_freq</th>\n",
       "      <th>prev_token_freq</th>\n",
       "      <th>pair_token_freq</th>\n",
       "      <th>lora_prob</th>\n",
       "      <th>full_prob</th>\n",
       "      <th>base_prob</th>\n",
       "      <th>full_lora_diff</th>\n",
       "      <th>lora_base_diff</th>\n",
       "      <th>full_base_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4983</th>\n",
       "      <td>PEC</td>\n",
       "      <td>and no diagonal that accepted 2 inch eyepiece...</td>\n",
       "      <td>129.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>Ċ</td>\n",
       "      <td>-4.907</td>\n",
       "      <td>27.0</td>\n",
       "      <td>79130.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.949</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.949</td>\n",
       "      <td>0.051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>-</td>\n",
       "      <td>iance occurred when light passed from air to w...</td>\n",
       "      <td>129.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>ref</td>\n",
       "      <td>-4.013</td>\n",
       "      <td>22036.0</td>\n",
       "      <td>1195.0</td>\n",
       "      <td>30106.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.015</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>107</td>\n",
       "      <td>2)     0.0005 (15)    0.0052 (16)    0.0093 (1...</td>\n",
       "      <td>129.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>.</td>\n",
       "      <td>-4.589</td>\n",
       "      <td>36.0</td>\n",
       "      <td>72634.0</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10475</th>\n",
       "      <td>gre</td>\n",
       "      <td>outrun the hare.\\n\\nIn the night\\n\\nhis eyes ...</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Ċ</td>\n",
       "      <td>-4.600</td>\n",
       "      <td>22.0</td>\n",
       "      <td>79130.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.198</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9333</th>\n",
       "      <td>edge</td>\n",
       "      <td>_Tapiola_\\n\\nHe is no more dead than Finland ...</td>\n",
       "      <td>129.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>Ċ</td>\n",
       "      <td>-4.274</td>\n",
       "      <td>22.0</td>\n",
       "      <td>79130.0</td>\n",
       "      <td>1533.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.677</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.677</td>\n",
       "      <td>0.323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      curr_token                                          in_tokens  \\\n",
       "4983         PEC   and no diagonal that accepted 2 inch eyepiece...   \n",
       "566            -  iance occurred when light passed from air to w...   \n",
       "966          107  2)     0.0005 (15)    0.0052 (16)    0.0093 (1...   \n",
       "10475        gre   outrun the hare.\\n\\nIn the night\\n\\nhis eyes ...   \n",
       "9333        edge   _Tapiola_\\n\\nHe is no more dead than Finland ...   \n",
       "\n",
       "       context_len  token_in_context  uniq_ctxt_tkns_count prev_token    pmi  \\\n",
       "4983         129.0               0.0                 102.0          Ċ -4.907   \n",
       "566          129.0               1.0                  74.0        ref -4.013   \n",
       "966          129.0               0.0                  38.0          . -4.589   \n",
       "10475         67.0               0.0                  44.0          Ċ -4.600   \n",
       "9333         129.0               0.0                  86.0          Ċ -4.274   \n",
       "\n",
       "       curr_token_freq  prev_token_freq  pair_token_freq  lora_prob  \\\n",
       "4983              27.0          79130.0            999.0        0.0   \n",
       "566            22036.0           1195.0          30106.0        0.0   \n",
       "966               36.0          72634.0           1680.0        0.0   \n",
       "10475             22.0          79130.0           1106.0        0.0   \n",
       "9333              22.0          79130.0           1533.0        0.0   \n",
       "\n",
       "       full_prob  base_prob  full_lora_diff  lora_base_diff  full_base_diff  \n",
       "4983         1.0      0.949             1.0          -0.949           0.051  \n",
       "566          1.0      0.015             1.0          -0.015           0.985  \n",
       "966          1.0      1.000             1.0          -1.000          -0.000  \n",
       "10475        1.0      0.198             1.0          -0.198           0.802  \n",
       "9333         1.0      0.677             1.0          -0.677           0.323  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected = pretrain_df[(pretrain_df[\"pmi\"] > -5) & (pretrain_df[\"pmi\"] < -4)]\n",
    "\n",
    "# find unique pairs of w_{i-1}, w_i\n",
    "agg_fns = {c: 'first' for c in pretrain_df.columns if c not in [\"curr_token\"]}\n",
    "agg_fns[\"full_lora_diff\"] = \"max\"\n",
    "df_uniq = pretrain_df.groupby([\"curr_token\"]).agg(agg_fns).reset_index().sort_values(\"full_lora_diff\", ascending=False)\n",
    "\n",
    "top_100 = df_uniq[:100]\n",
    "\n",
    "top_100.loc[:, [\"pmi\", \"curr_token_freq\", \"lora_prob\", \"full_prob\", \"base_prob\", \"full_lora_diff\", \"lora_base_diff\", \"full_base_diff\"]] = top_100[[\"pmi\", \"curr_token_freq\", \"lora_prob\", \"full_prob\", \"base_prob\", \"full_lora_diff\", \"lora_base_diff\", \"full_base_diff\"]].round(3)\n",
    "top_100.to_csv(\"results/examples_ft_pt/pretrain_data_100.csv\", index=False)\n",
    "top_100.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge pretrain/finetune data with finetune/pretrain token frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_data_stats = pretrain_df[[\"prev_token\", \"curr_token\", \"curr_token_freq\", \"prev_token_freq\", \"pair_token_freq\", \"pmi\"]]\n",
    "ft_data_stats = finetune_df[[\"prev_token\", \"curr_token\", \"curr_token_freq\", \"prev_token_freq\", \"pair_token_freq\", \"pmi\"]]\n",
    "\n",
    "pt_data_stats = pt_data_stats.add_prefix('pt_').drop_duplicates().reset_index(drop=True)\n",
    "pt_data_stats = pt_data_stats.rename(columns={\"pt_prev_token\": \"prev_token\", \"pt_curr_token\": \"curr_token\"})\n",
    "ft_data_stats = ft_data_stats.add_prefix('ft_').drop_duplicates().reset_index(drop=True)\n",
    "ft_data_stats = ft_data_stats.rename(columns={\"ft_prev_token\": \"prev_token\", \"ft_curr_token\": \"curr_token\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_pairs = pt_data_stats[[\"curr_token\", \"prev_token\", \"pt_pmi\"]]\n",
    "ft_pairs = ft_data_stats[[\"curr_token\", \"prev_token\", \"ft_pmi\"]]\n",
    "\n",
    "common_pairs = pd.merge(pt_pairs, ft_pairs, on=[\"curr_token\", \"prev_token\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_token_freq = pt_data_stats[[\"curr_token\", \"pt_curr_token_freq\"]]\n",
    "pt_token_freq = pt_token_freq.drop_duplicates().reset_index(drop=True)\n",
    "finetune_df_merged = pd.merge(finetune_df, pt_token_freq, on=[\"curr_token\"], how=\"left\")\n",
    "finetune_df_merged[\"pt_curr_token_freq\"] = finetune_df_merged[\"pt_curr_token_freq\"].fillna(0)\n",
    "finetune_df_merged = finetune_df_merged.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "ft_token_freq = ft_data_stats[[\"curr_token\", \"ft_curr_token_freq\"]]\n",
    "ft_token_freq = ft_token_freq.drop_duplicates().reset_index(drop=True)\n",
    "pretrain_df_merged = pd.merge(pretrain_df, ft_token_freq, on=[\"curr_token\"], how=\"left\")\n",
    "pretrain_df_merged[\"pt_curr_token_freq\"] = pretrain_df_merged[\"ft_curr_token_freq\"].fillna(0)\n",
    "pretrain_df_merged = pretrain_df_merged.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_df_merged.to_csv(\"results/data/finetune_data_probs.csv\", index=False)\n",
    "pretrain_df_merged.to_csv(\"results/data/pretrain_data_probs.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lora_mem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
